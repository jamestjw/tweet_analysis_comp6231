\documentclass[a4paper,12pt]{article}

\usepackage{graphicx}
\usepackage{url}
\usepackage{xcolor}

\setlength{\parskip}{\baselineskip}%
\setlength{\parindent}{0pt}%

\begin{document}

\title{Large Scale Sentiment Analysis of Tweets }%replace with the appropriate homework number
\author{
	James Tan Juan Whei\\
	Concordia University
}  
\maketitle

\begin{abstract}
Abstract, here is what an abstract is compared to an intro

https://www.discoverphds.com/blog/abstract-vs-introduction
\end{abstract}

\section{Introduction}

This project aims to apply distributed system concepts with real applications, such as NoSQL systems for big data,
and demonstrate the power of distributed systems in extracting and analyzing the general sentiment expressed in a large
data set of tweets, and detecting emotions and cyberbullying using pretrained Spark NLP DL classifiers.
Emotions identified include: Joy, Surprise, Fear, Sadness while detecting Racism, Sexism or Neutral tweets.


\section{Tools and Technologies}
Given the large dataset, approximately 10 GB of scrapped tweets, we chose google cloud storage.
Google Cloud Storage is a RESTful online file storage web service for storing and accessing data on a Google Cloud
Platform infrastructure. The service combines the performance and scalability of the cloud with advanced
security and sharing capabilities, all at no cost to a user using less than 15 GB of cloud space for free.

Furthermore, Big Query was chosen to store the tweets as tabular data in datasets and tables. BigQuery is a data warehouse
and a SQL Engine that enabled ease of data import due to it's compatibility with GCS. It's also an affordable big data
warehouse, where the first Terabyte of data processed each month is free.

In order to process our data and draw insight from processed tweets, we used Apache Spark, an open-source unified analytics engine for
large-scale data processing. To run Apache Spark, we used Dataproc, given the fast, easy-to-use, and fully managed cloud
service for running Apache Spark clusters in a simpler, more cost-efficient way. It helps you create clusters quickly,
manage them easily, and save money by turning clusters off when you don't need them.

\subsection{GCS}
Some description about GCS

\subsection{BigQuery}
Some description about BigQuery

\subsection{Dataproc and Spark}
\label{sec:spark}

Dataproc is a managed framework that runs on the Google Cloud Platform and ties together several popular tools for processing data. 
We used it to run Apache Spark clusters. It's easy-to-use and relatively fast, as you can spin up a cluster in about 90 seconds.
It’s cheaper than building your own cluster because you can spin up a Dataproc cluster when you need to run a job and
shut it down afterward, so you only pay when jobs are running. Dataproc is priced at only 1 cent per virtual CPU in your
cluster per hour. Conveniently, it’s integrated with other Google Cloud services, including Cloud Storage, BigQuery, and
Cloud Bigtable, making it easy to transfer our data. Due to performance considerations as well as the high reliability of
storage attached to Dataproc clusters, the default replication factor is set at 2.

Compute Engine Virtual Machine instances (VMs) in a Dataproc cluster, consisting of master and worker
virtual machines, require full internal IP networking cross connectivity. The default Virtual Private Cloud network provides
this connectivity, and offers native Internal TCP/UDP Load Balancing and proxy systems for Internal
HTTP and HTTPS Load Balancing.

Dataproc clusters can be created using the Google Cloud Platform console, or alternatively to create a Dataproc cluster
on the command line, run the Cloud SDK gcloud dataproc clusters create command locally in a terminal window or in
Cloud Shell. Refer to section 7 - Code to see our code for cluster creation.

Basic requirements for cluster creation include specifying a cluster name, region (global or a specific region for the cluster), and
connectivity consisting of the number of master and worker nodes.

gcloud dataproc clusters create cluster-name \
--region=region

The above command creates a cluster with default Dataproc service settings for your master and worker virtual machine
instances, disk sizes and types, network type, region and zone where your cluster is deployed, and other cluster settings.

In order to set attributes specific to the required resource, you can specify the specific values against the
flag you want to modify.

The following flags were used to specify unique attributes for the resources of our VMs:

Type of machine to use for worker nodes.

--worker-machine-type n1-standard-8 \

The size of the boot disk, in our case a 128 GB disk.

--master-boot-disk-size=128GB \
--worker-boot-disk-size=128GB \

We used the default number of master nodes (1 node), however, to specify otherwise when High Availability is critical,
the number of master nodes in the cluster can be changed to 3.

--num-masters=NUM_MASTERS

The number of worker nodes in the cluster, we only used 2 worker nodes.

--num-workers=NUM_WORKERS \

To submit a job to a Dataproc cluster, run the gcloud CLI gcloud dataproc jobs submit command locally
in a terminal window or in Cloud Shell.

\section{Dataset}
\label{sec:dataset}
We relied heavily on the snscrape library\footnote{Scraper for social networking services (SNS): https://github.com/JustAnotherArchivist/snscrape} to scrape tweets. We filtered for tweets that were in the English language only. Approximately 1000000 tweets were scraped for each day between 2022-05-01 and 2022-07-12. The tweets were then saved in Google Cloud Storage.

\section{Processing of Data}

As mentioned in Section \ref{sec:spark}, we define the processing of the data as a Spark job. The steps involved in the job are illustrated in Figure \ref{fig:data-processing-pipeline} and will be further elaborated on in the subsequent sections. Our processing pipeline relies heavily on the Spark NLP library\footnote{Spark NLP: State of the Art Natural Language Processing: https://nlp.johnsnowlabs.com}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{data-processing.png}
\caption{The processing pipeline}
\label{fig:data-processing-pipeline}
\end{figure}

\subsection{Data Ingestion}
In order to use data stored in BigQuery as an input to our Spark job, we used the Spark BigQuery connector\footnote{Use the BigQuery connector with Spark: https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example}. The Spark script reads from a table that contains all the tweets that were procured as described in Section \ref{sec:dataset}. 

Each run of the Spark job would typically be executed on 4-5 days' worth of tweets as we discovered that the Spark jobs had a tendency of failing when working with larger amounts of data. This was true even when the CPU and memory utilization of the worker nodes were relatively healthy and thus should be further investigated.

\subsection{Document Assembler}
The first step of the pipeline is the \texttt{DocumentAssembler}\footnote{https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/DocumentAssembler}. This prepares the data into a format that is processable by Spark NLP and is essentially the entry point for every Spark NLP pipeline.

\subsection{Generation of Sentence Embeddings}
\label{sec:sentence-embeddings}
We generate sentence embeddings by leveraging a Universal Sentence Encoder\footnote{https://www.tensorflow.org/hub/tutorials/semantic\_similarity\_with\_tf\_hub\_universal\_encoder?hl=en} made available by Tensorflow. The output of this stage is a 512-dimensional vector that semantically captures the meaning of each tweet. This is the basis upon which the downstream classification algorithms build on.

\subsection{Sentiment Classification}
To actually use the embeddings described in the previous section, we utilise ClassifierDLModels\footnote{https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/ClassifierDLModel.html} to classify the tweets. Each ClassifierDLModels essentially assigns a label to each tweet. To identify the emotion, presence of cyberbullying and presence of racism in each tweet, we use the \texttt{classifierdl\_use\_emotion}, \texttt{classifierdl\_use\_cyberbullying} and \texttt{classifierdl\_use\_sarcasm} pretrained models respectively ({\color{red}TODO: Better way of phrasing this?}). 

The emotion classifier produces the values \texttt{sadness}, \texttt{joy}, \texttt{love}, \texttt{anger}, \texttt{fear} and \texttt{surprise}. The cyberbullying classifier produces the values \texttt{neutral}, \texttt{racism} and \texttt{sexism}. The sarcasm classifier produces the values \texttt{sarcasm} and \texttt{normal}. 

\subsection{Storing of Output}
The output is then stored in a separate table in BigQuery. Note that the BigQuery Spark connector is once again used here, thus allowing the output of a Spark job to be appended directly to a BigQuery table.

\section{Results}

To visualize the general sentiment expressed in tweets, detected emotions and cyberbullying, we used Jupyter notebook to
display and provide functionality to query the data interactively.

Emotions identified include: Joy, Surprise, Fear, Sadness while detecting Racism, Sexism or Neutral tweets.

Below you see the sample output for a query on racist tweets between the 1st and 4th of May, 2022. Note, the data output
has been cropped to 7 tweets for convenience.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{racism.png}
	\caption{Sample Query: Racist Tweets}
	\label{fig:Racist-Tweet-Query}
\end{figure}

Below is another visualization provided using Jupyter notebook, that enables us to visualize the emotions of tweets over time.
Interestingly, you can see the spike of fear on May 24th, 2022.

\begin{figure}[h]
{\centering
	\includegraphics[width=\textwidth]{graph.png}
	\caption{Sample Query: General Sentiment Over Time}
	\label{fig:Emotion-Time-Query}
}
	\vspace{5mm} %5mm vertical space
By using our interactive tweet-emotion visualizer, we can
observe the emotion of the general users on twitter, and here we unfortunately notice that a shooting had taken place
on the 24th of May, 2022.
	{
	\centering
	\includegraphics[width=\textwidth]{graphTweets.png}
	\caption{Sample Query: Fear}
	\label{fig:Fear-Query}
}
\end{figure}

\section{Future Work}
An idea that we originally had was to run an unsupervised clustering algorithm on the sentence embeddings produced in Section \ref{sec:sentence-embeddings}. The clustering algorithm that we had in mind was DBScan\cite{Ester96adensity-based}, with the objective of clustering tweets with similar topics. Unfortunately, we were not able to find a satisfactorily efficient implementation of the algorithm to employ with Spark. Due to the size of our dataset and the high number of dimensions of the embeddings, an efficient implementation was crucial to the success of this idea. Given that we had limited resources, we were forced to abandon this idea, however, it should be revisited in the future by implementing our own version of the DBScan algorithm.

\section{Code}
The code to reproduce our project may be found in our Github repository.\footnote{Github repository of the project: https://github.com/jamestjw/tweet\_analysis\_comp6231} 

\section{Conclusion}
We worked hard and achieved very little.

\nocite{*}

\bibliographystyle{abbrv}
\bibliography{main.bib}

\end{document}  
