# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NILyaS7PBklsZqNCBTe0QhaZGdO40MSr

# Setup Spark
"""

from pyspark.sql import SparkSession
import dbscan
from scipy.spatial.distance import euclidean
import pyspark.sql.functions as F

DIM = 256

spark = (
    SparkSession.builder.appName("clusteringtest").config(
        "spark.jars.packages", "graphframes:graphframes-0.8.2-spark3.1-s_2.12"
    )
    # This line may or may not be necessary
    .getOrCreate()
)

"""# DBScan"""

"""## Reading in dataset"""

data_filename_stem = "tweet_dump_2022-06-16"
file_dir = (
    f"gs://tweet_analysis/embeddings/bert-embeddings-{data_filename_stem}.csv.parquet"
)

embeddings = (
    spark.read.parquet(file_dir)
    .select(F.col("tweet_id").alias("id"), F.col("embeddings").alias("value"))
    .filter(F.size(F.col("value")) == 256)
)

# TODO: Fix this, doesnt complete for some reason
res = dbscan.process(
    spark,
    embeddings,
    epsilon=50,
    min_pts=20,
    dist=euclidean,
    dim=DIM,
    checkpoint_dir="./checkpoint",
    operations=None,
)

output_dir = f"gs://tweet_analysis/clustering/output/clusters-{data_filename_stem}.csv"
res.write.csv(output_dir)
